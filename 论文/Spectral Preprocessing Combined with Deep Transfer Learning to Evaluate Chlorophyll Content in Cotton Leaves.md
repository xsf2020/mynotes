#传统回归模型
## 1. 偏最小二乘回归 (Partial Least Squares, PLS)

PLS 的精髓在于：它不是简单的线性回归，也不是简单的降维，而是**“带方向的降维”**。

### 数学原理

普通的最小二乘法 (OLS) 在变量太多或变量间高度相关时会“崩溃”（系数变得不稳定）。PLS 借鉴了主成分分析 (PCA) 的思想，但有所不同：

- **PCA (主成分分析)**：只管从自变量 $X$ 中找方差最大的方向（只看 $X$ 内部）。
    
- **PLS**：同时对 $X$ 和 $Y$ 进行分解。它寻找的特征向量（称为**潜在变量 Latent Variables**），必须同时满足两个条件：
    
    1. 能尽可能多地保留 $X$ 中的信息。
        
    2. 与 $Y$ 的相关性达到最大。
        

### 工作流程

1. **提取成分**：从原始变量 $X$ 中线性组合出第 1 个潜在变量 $t_1$。
    
2. **建立关联**：计算 $t_1$ 与 $Y$ 的关系。
    
3. **残差更新**：从 $X$ 和 $Y$ 中剔除 $t_1$ 已经解释掉的部分，用剩下的残差继续提取 $t_2$，直到达到预设的主成分个数。
    
4. **重构模型**：最后将所有的潜在变量转换回原始 $X$ 变量的系数。
    

### 核心优势

- **处理多重共线性**：即使变量之间高度相关（如近红外光谱的连续波段），PLS 也能稳定提取信息。
    
- **高维小样本**：当特征数 $p$ 远大于样本数 $n$ 时（例如 1000 个特征，只有 20 个实验样本），PLS 依然有效。
    
- **物理意义**：通过查看变量投影重要性 (VIP) 分数，可以明确知道哪些特征对预测最关键。


## 2. 支持向量机回归 (SVR)

SVR 的精髓在于：它对小的误差“视而不见”，只关注那些“出格”的样本，并能通过“空间跳跃”处理非线性。

### 数学原理

SVR 与传统回归追求“误差平方和最小”不同，它引入了一个**$\epsilon$-不敏感带 (Epsilon-tube)**：

- 如果预测值与真实值的差距在 $\epsilon$ 以内，模型认为没有损失（Loss = 0）。
    
- 只有当预测点掉出这个“管子”时，才计算损失。
    
- **核技巧 (Kernel Trick)**：这是 SVR 的灵魂。它通过核函数（如 RBF 径向基核）将原始数据映射到更高维的空间。在这个高维空间里，原本复杂的非线性关系会变成线性的，从而实现精准拟合。
    

### 工作流程

1. **选择核函数**：根据数据分布选择线性核、多项式核或 RBF 核。
    
2. **训练优化**：寻找一个超平面，使得尽量多的样本点落在 $\epsilon$ 隔离带内，同时保证超平面平滑（防止过拟合）。
    
3. **确定支持向量**：最终模型只由落在边界上或边界外的少数样本（支持向量）决定，大部分带内的点对模型没有贡献。
    

### 核心优势

- **处理非线性**：通过核函数，SVR 能捕捉到 PLS 这种线性模型无法处理的复杂曲线关系。
    
- **鲁棒性**：对异常值不敏感，因为只有部分样本（支持向量）决定模型。
    
- **泛化能力**：由于其追求的是“结构风险最小化”（即模型既要准，又要简单），通常比普通线性回归更不容易过拟合。
![](assets/Spectral%20Preprocessing%20Combined%20with%20Deep%20Transfer%20Learning%20to%20Evaluate%20Chlorophyll%20Content%20in%20Cotton%20Leaves/file-20260122140015183.png)